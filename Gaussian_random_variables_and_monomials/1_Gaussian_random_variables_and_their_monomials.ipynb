{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a5539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b3fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib import font_manager\n",
    "plt.rcParams['font.family'] = 'Helvetica'\n",
    "plt.rcParams['font.weight'] = 'light'\n",
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams[\"axes.labelweight\"] = \"light\"\n",
    "plt.rcParams[\"axes.labelsize\"] = 17\n",
    "plt.rcParams[\"axes.linewidth\"] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee87500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm.any == 0: \n",
    "        return v\n",
    "    return v / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39963c5e",
   "metadata": {},
   "source": [
    "# I) 10 Gaussian random variables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4792490b",
   "metadata": {},
   "source": [
    "## Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb03a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1500 #points\n",
    "d=10 #dimension\n",
    "cov = np.identity(10)\n",
    "mean = np.zeros(10)\n",
    "np.random.seed(seed=9)\n",
    "Xori = np.random.multivariate_normal(mean = mean, cov = cov, size = (N))  #unweighted input\n",
    "weights=np.ones(10)*(0.01**2)\n",
    "weights[0:5]=[5,2,1,1,0.5]\n",
    "weights\n",
    "X = Xori*weights #weighted target/ ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898850ea",
   "metadata": {},
   "source": [
    "## Produce / Import the results which where optimized in two settings: \n",
    "- with exponentially decaying learning rate and \n",
    "- with cosine decaying learning rate\n",
    "\n",
    "These optimizations take 1-2 hours on a regular work station, because we consider relatively many points (1500) and optimize over extremely many periods. In general, far less periods (< 100) are suitable. \n",
    "\n",
    "If you want to reproduce the optimization, please uncomment the following cell. Otherwise skip it and proceed with the following cells, loading our saved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08323950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The L1 penalty strengths of the optimizations\n",
    "\n",
    "lassos_ori = np.array([0] + list(\n",
    "    np.logspace(\n",
    "        np.floor(np.log10((1 / 50.) / 1000)),\n",
    "        np.ceil(np.log10((1 / 50.) * 1.5)),\n",
    "        25,\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985da2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimization\n",
    "\n",
    "# n_epochs = 800\n",
    "# f = FeatureWeighting(coordinates=Xori, verbose=True)\n",
    "# f_target = FeatureWeighting(coordinates=X)\n",
    "# optilr = f.return_optimal_learning_rate(target_data = f_target, n_epochs = 100, n_samples=700, trial_learning_rates=np.array([ 10., 50., 100., 200.]))\n",
    "\n",
    "# # Exponentially decaying learning rate\n",
    "# (\n",
    "#     num_exp,\n",
    "#     lassos_exp,\n",
    "#     ii0_exp,\n",
    "#     ww0_exp,\n",
    "# ) = f.return_lasso_optimization_dii_search(\n",
    "#     target_data=f_target,\n",
    "#     l1_penalties=lassos,\n",
    "#     initial_weights=None,  # (default) set automatically\n",
    "#     n_epochs=n_epochs,\n",
    "#     learning_rate=50., \n",
    "#     decaying_lr=\"exp\",\n",
    "#     refine=False,  # only 10 values of the L1 strength are tested\n",
    "#     plotlasso=True,  # automatically show DII vs number of non-zero features\n",
    "# )\n",
    "# ii0_perl1_perepoch_exp = f.history['dii_per_l1_per_epoch']\n",
    "# ww0_perl1_perepoch_exp = f.history['weights_per_l1_per_epoch']\n",
    "# # np.save('./PLOTDATA/num_nonzero_small',num_exp)\n",
    "# # np.save('./PLOTDATA/l1o_small',lassos_exp)\n",
    "# # np.save('./PLOTDATA/diio_small',ii0_exp)\n",
    "# # np.save('./PLOTDATA/weightso_small',ww0_exp)\n",
    "# # np.save('./PLOTDATA/dii_per_l1_per_epoch_small', ii0_perl1_perepoch_exp)\n",
    "# # np.save('./PLOTDATA/weights_per_l1_per_epoch_small', ww0_perl1_perepoch_exp)\n",
    "\n",
    "# # Cosine decaying learning rate\n",
    "# (\n",
    "#     num_cos,\n",
    "#     lassos_cos,\n",
    "#     ii0_cos,\n",
    "#     ww0_cos,\n",
    "# ) = f.return_lasso_optimization_dii_search(\n",
    "#     target_data=f_target,\n",
    "#     l1_penalties=lassos,\n",
    "#     initial_weights=None,  # (default) set automatically\n",
    "#     n_epochs=n_epochs,\n",
    "#     learning_rate=50., \n",
    "#     decaying_lr=\"cos\",\n",
    "#     refine=False,  # only 10 values of the L1 strength are tested\n",
    "#     plotlasso=True,  # automatically show DII vs number of non-zero features\n",
    "# )\n",
    "# ii0_perl1_perepoch_cos = f.history['dii_per_l1_per_epoch']\n",
    "# ww0_perl1_perepoch_cos = f.history['weights_per_l1_per_epoch']\n",
    "# # np.save('./PLOTDATA/COS/num_nonzero_small',num_cos)\n",
    "# # np.save('./PLOTDATA/COS/l1o_small',lassos_cos)\n",
    "# # np.save('./PLOTDATA/COS/diio_small',ii0_cos)\n",
    "# # np.save('./PLOTDATA/COS/weightso_small',ww0_cos)\n",
    "# # np.save('./PLOTDATA/COS/dii_per_l1_per_epoch_small', ii0_perl1_perepoch_cos)\n",
    "# # np.save('./PLOTDATA/COS/weights_per_l1_per_epoch_small', ww0_perl1_perepoch_cos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b35a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "#EXPONENTIAL decay results\n",
    "ii0_exp = np.load(\"./PLOTDATA/diio_small.npy\") # DIIs for each number of non-zero features\n",
    "ww0_exp = np.load(\"./PLOTDATA/weightso_small.npy\") # according weights\n",
    "lassos_exp = np.load(\"./PLOTDATA/l1o_small.npy\") # the L1 penalty strength for above results\n",
    "num_exp = np.load(\"./PLOTDATA/num_nonzero_small.npy\") # the available numbers of non-zero features\n",
    "ii0_perl1_perepoch_exp = np.load(\"./PLOTDATA/dii_per_l1_per_epoch_small.npy\") # the optimization for each ii0_exp\n",
    "ww0_perl1_perepoch_exp = np.load(\"./PLOTDATA/weights_per_l1_per_epoch_small.npy\") # the optimization for each set of ww0_exp\n",
    "\n",
    "#COSINE decay results\n",
    "ii0_cos = np.load(\"./PLOTDATA/COS/diio_small.npy\")\n",
    "ww0_cos = np.load(\"./PLOTDATA/COS/weightso_small.npy\")\n",
    "lassos_cos = np.load(\"./PLOTDATA/COS/l1o_small.npy\")\n",
    "num_cos = np.load(\"./PLOTDATA/COS/num_nonzero_small.npy\")\n",
    "ii0_perl1_perepoch_cos = np.load(\"./PLOTDATA/COS/dii_per_l1_per_epoch_small.npy\")\n",
    "ww0_perl1_perepoch_cos = np.load(\"./PLOTDATA/COS/weights_per_l1_per_epoch_small.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c9f83",
   "metadata": {},
   "source": [
    "## Merge the two sets of results\n",
    "According to lowest DII if they have the same number of non-zero features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e070d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercollate exponential decay resulting DIIs and cosine annealing ones (for Fig.1 AIII)\n",
    "\n",
    "#deep copies of the arrays (by using 1*)\n",
    "ii0 = 1*ii0_exp #DIIs\n",
    "ww0 = 1*ww0_exp #weights\n",
    "lassos0 = 1*lassos_exp #L1 strengths\n",
    "num_nonzero_features0 = 1*num_exp\n",
    "decaytype0 = np.full(ii0.shape, 0.) #0 is exponential decay\n",
    "decaytype0[np.where(np.isnan(ii0))] = np.nan\n",
    "\n",
    "for i, number in enumerate(num_exp):\n",
    "    if (num_cos[i] == number and ii0_cos[i] < ii0_exp[i]) or (np.isnan(num_exp[i]) and not np.isnan(num_cos[i])):\n",
    "        decaytype0[i] = 1. # 1 means cosine annealing decay\n",
    "        ii0[i] = 1*ii0_cos[i]\n",
    "        ww0[i] = 1*ww0_cos[i]\n",
    "        lassos0[i] = 1*lassos_cos[i]\n",
    "        num_nonzero_features0[i] = 1*num_cos[i]\n",
    "        \n",
    "\n",
    "# Extract the dii and weight optimizations for each of the non-zero feature results (for Fig.1 AI)\n",
    "\n",
    "# find indices of L1 strengths used for the results?\n",
    "lassoindices = np.ones(ii0.shape) * np.nan\n",
    "for i, lasso in enumerate(lassos0):\n",
    "    j = np.where(np.round(lassos_ori,7) == np.round(lasso,7))[0]\n",
    "    if j.size != 0:\n",
    "        lassoindices[i] = j[0]\n",
    "# now extract the correct optimizations of diis and weights:\n",
    "epochs= ii0_perl1_perepoch_exp.shape[1]\n",
    "features = ww0_perl1_perepoch_exp.shape[2]\n",
    "ii_evolutions = np.ones((len(ii0), epochs)) * np.nan\n",
    "ww_evolutions = np.ones((len(ii0), epochs, features)) * np.nan\n",
    "\n",
    "for i, lassind in enumerate(lassoindices):\n",
    "    if decaytype0[i] == 0.: #extract from the exponential decay\n",
    "        ii_evolutions[i] = ii0_perl1_perepoch_exp[int(lassind)]\n",
    "        ww_evolutions[i] = ww0_perl1_perepoch_exp[int(lassind)]\n",
    "    elif decaytype0[i] == 1.: #extract from the cosine decay\n",
    "        ii_evolutions[i] = ii0_perl1_perepoch_cos[int(lassind)]\n",
    "        ww_evolutions[i] = ww0_perl1_perepoch_cos[int(lassind)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afb6628",
   "metadata": {},
   "source": [
    "## overlap (cosine similarity) between groundtruth and lasso results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_0 = []\n",
    "for i in range(len(ww0)):\n",
    "    overl = np.dot(normalize(ww0[i]), normalize(weights))\n",
    "    overlap_0.append(overl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1376922c",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9347ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import NullFormatter\n",
    "fig,ax = plt.subplots(figsize=(4.5, 3.5))\n",
    "features = [0,5,6,7,8] # selected plot points\n",
    "\n",
    "# Overlaps\n",
    "ov = np.array(overlap_0)[features]\n",
    "nn0= np.linalg.norm(ww0[features], ord=0, axis=1).astype(int)\n",
    "ax.plot(nn0, ov, \"-o\",color=\"tab:gray\", linewidth=1)\n",
    "ax.set_xlabel(\"Number of non-zero features\")\n",
    "ax.set_ylabel(\"Overlap of weights\",color=\"tab:gray\")\n",
    "ax.tick_params(axis='y', colors=\"tab:gray\")\n",
    "ax.set_ylim(0.92,1.005)\n",
    "ax.invert_xaxis()\n",
    "plt.yticks([0.93,0.96,0.99])\n",
    "\n",
    "\n",
    "# DIIs\n",
    "endimbalances = ii0[features]\n",
    "ax2=ax.twinx()\n",
    "ax2.plot(nn0, endimbalances, \"-\",color=\"black\", linewidth=1, zorder=1)\n",
    "colors = ['tab:blue', 'tab:green', 'tab:red', 'tab:orange', 'tab:cyan']\n",
    "counter=0\n",
    "for y, c in zip(endimbalances, colors):\n",
    "    plt.scatter(nn0[counter], y, color=c)\n",
    "    counter +=1\n",
    "ax2.set_ylabel(\"DII\")\n",
    "ax2.set_ylim(-0.01,0.1)\n",
    "plt.xticks([10,8,6,4,2])\n",
    "plt.yticks([0.,0.04,0.08])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the base (without L1 regularization) and L1=0.0068 results\n",
    "\n",
    "till=20 # length of plot axis\n",
    "\n",
    "# L1=0.0068\n",
    "num = 7 # index of L1=0.0068\n",
    "besti0 = 1*ii_evolutions[num][:till]\n",
    "besti0[till-10:till] = np.nan\n",
    "besti0[till-1] = 1*ii_evolutions[num][800]\n",
    "# to connect trough missing values:\n",
    "bestimask0 = np.isfinite(besti0)\n",
    "\n",
    "# Unregularized\n",
    "base = 1*ii_evolutions[0][:till]\n",
    "base[till-10:till] = np.nan\n",
    "base[till-1] = 1*ii_evolutions[0][800]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3.5)) \n",
    "# Unregularized\n",
    "plt.plot(np.arange(len(base))[bestimask0], base[bestimask0], 'o--', linewidth=1., color=\"tab:blue\")\n",
    "plt.plot(base[:till-9], 'o-',label=lassos0[0], linewidth=1.5, color=\"tab:blue\")\n",
    "# L1=0.007\n",
    "plt.plot(np.arange(len(besti0))[bestimask0], besti0[bestimask0], 'o--', linewidth=1., color=\"tab:orange\")\n",
    "plt.plot(besti0[:till-9], 'o-',label=np.round(lassos0[num],4), linewidth=1.5, color=\"tab:orange\")\n",
    "\n",
    "plt.legend(title = \"L$_1$ strength\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"DII\")\n",
    "plt.xlim(-1,till)\n",
    "plt.xticks([0,5,10,20], labels=[0,5,10,800])\n",
    "plt.yticks([0.,0.1,0.2])\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d28887",
   "metadata": {},
   "source": [
    "### the insets of the data point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2436ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting the insets in Fig.1 AI, make pandas frames of the data:\n",
    "dims = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10']\n",
    "Xpd = pd.DataFrame(X, columns=dims)\n",
    "Xoripd = pd.DataFrame(Xori, columns=dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ed0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the correct scaling weights for the point clouds\n",
    "\n",
    "gtvector0 = 1*weights\n",
    "# Normalized weights\n",
    "weightslist_gt_0_10_50 = []\n",
    "weightslist_gt_0_10_50.append(normalize(gtvector0)) # normalized gt weights\n",
    "#also the weights of the beginning, epoch 1 and epoch 3:\n",
    "optipoints = [0, 1, 3]\n",
    "for i in ww_evolutions[num][optipoints]:\n",
    "    weightslist_gt_0_10_50.append(normalize(i))\n",
    "    \n",
    "pandaslist0 = []\n",
    "for i in weightslist_gt_0_10_50:\n",
    "    Xori_subset = Xori * i\n",
    "    Xori_subsetpd = pd.DataFrame(Xori_subset[:,[0,3]], columns=['$X_1$', '$X_4$']) #np.array(nameslist)[[20,47]]\n",
    "    pandaslist0.append(Xori_subsetpd)\n",
    "\n",
    "# Plot\n",
    "\n",
    "for i in range(1,len(optipoints)+1):\n",
    "    concatenated = pd.concat([pandaslist0[0].assign(Weights='Ground truth'), pandaslist0[i].assign(Weights='Optimized')])\n",
    "    #sns.scatterplot(x='Std', y='ATR', data=concatenated, hue='Asset Subclass', style='dataset')\n",
    "\n",
    "    alphas = np.ones(3000)*1\n",
    "    alphas[:1500] = 0.3\n",
    "    g = sns.JointGrid(data=concatenated, x=\"$X_1$\", y=\"$X_4$\", height=2, xlim=(-4,4), ylim=(-4,4), hue='Weights', palette=['tab:gray','tab:orange'])\n",
    "    g.plot_joint(sns.scatterplot, s=30, alpha=alphas, linewidth=0)\n",
    "    g.plot_marginals(sns.kdeplot, fill=True, alpha=0.2, linewidth=2)\n",
    "    g.ax_joint.tick_params(left=False, bottom=False, labelbottom=False, labelleft=False)\n",
    "    g.ax_joint.legend_.remove()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc8cf41",
   "metadata": {},
   "source": [
    "### for the table in the Fig.1 A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8358162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [0,5,6,7,8]\n",
    "for i, feat in enumerate (features):\n",
    "    print(\"l1: \",np.round(lassos0[feat],5),\" norm: \", num_nonzero_features0[feat].astype(int), \" imb: \", np.round(ii0[feat],3), \" weights: \", (ww0[feat]/np.max(ww0[feat]))*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc19f0a4",
   "metadata": {},
   "source": [
    "# II) 285 polynomials of ten Gaussian random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7b35f2",
   "metadata": {},
   "source": [
    "## Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d693b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the new input data, all 285 polynomials\n",
    "ts = range(10)\n",
    "polylist = []\n",
    "for i in [1,2,3]:\n",
    "    polylist = polylist + list(itertools.combinations_with_replacement(ts,i))\n",
    "\n",
    "Xori2 = np.empty((Xori.shape[0],len(polylist))) # The unscaled input features\n",
    "for i, poly in enumerate(polylist):\n",
    "    data = np.prod(Xori[:, poly], axis=1)\n",
    "    Xori2[:,i] = data \n",
    "\n",
    "# the variable name combinations\n",
    "tsnames = ['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10']\n",
    "polylist = []\n",
    "for i in [1,2,3]:\n",
    "    polylist = polylist + list(itertools.combinations_with_replacement(tsnames,i))\n",
    "nameslist = ['*'.join(item) for item in polylist]\n",
    "\n",
    "# Ground truth: Random 10 features from these 285 and weights\n",
    "gtlist = [5,20,47,280,100,11,4,2,9,7]\n",
    "weights2 = [5,5,1,2,7,3,10,6,4,1]\n",
    "X2 = Xori2[:,gtlist] * weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c602df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip weight and variable name\n",
    "zipped = list(zip(np.array(weights2).astype(str), np.array(nameslist)[gtlist]))\n",
    "nameslist_gt = ['*'.join(item) for item in zipped]\n",
    "\n",
    "# Zip feature index, weight and name, sorted by weight\n",
    "order = np.argsort(weights2)[::-1]\n",
    "zipped = list(zip(np.array(weights2).astype(str)[order], np.array(nameslist)[gtlist][order]))\n",
    "list_gt = ['*'.join(item) for item in zipped]\n",
    "zipped2 = list(zip(np.array(gtlist).astype(str)[order], list_gt))\n",
    "list_gt = ['__'.join(item) for item in zipped2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285ae50a",
   "metadata": {},
   "source": [
    "## Produce / Import the results which where optimized in two settings: \n",
    "- with exponentially decaying learning rate and \n",
    "- with cosine decaying learning rate\n",
    "\n",
    "These optimizations take several hours on a regular work station (over night), because we consider relatively many points (1500) and optimize over extremely many periods. In general, far less periods (< 100) are suitable. \n",
    "\n",
    "If you want to reproduce the optimization, please uncomment the following cell. Otherwise skip it and proceed with the following cells, loading our saved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9439c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The L1 penalty strengths of the optimizations\n",
    "\n",
    "lassos_ori_big = [0] + list(\n",
    "    np.logspace(\n",
    "        np.floor(np.log10((1 / 200.) / 1000)),\n",
    "        np.ceil(np.log10((1 / 200.) * 1.5)),\n",
    "        20,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimizations\n",
    "# n_epochs = 800  # number of training epochs\n",
    "# ff = FeatureWeighting(coordinates=Xori2, verbose=True)\n",
    "# ff_target = FeatureWeighting(coordinates=X2)\n",
    "# optilr2 = ff.return_optimal_learning_rate(target_data = ff_target, n_epochs = 300, n_samples=600, trial_learning_rates=np.array([ 10., 50., 100., 200.]))\n",
    "\n",
    "# # Exponentially decaying learning rate\n",
    "# (\n",
    "#     num_exp,\n",
    "#     lassos_exp,\n",
    "#     ii0_exp,\n",
    "#     ww0_exp,\n",
    "# ) = ff.return_lasso_optimization_dii_search(\n",
    "#     target_data=ff_target,\n",
    "#     l1_penalties=largelassos,\n",
    "#     initial_weights=None,  # (default) set automatically\n",
    "#     n_epochs=n_epochs,\n",
    "#     learning_rate=optilr2, \n",
    "#     decaying_lr=\"exp\",\n",
    "#     refine=False,  # only 10 values of the L1 strength are tested\n",
    "#     plotlasso=True,  # automatically show DII vs number of non-zero features\n",
    "# )\n",
    "# # ii0_perl1_perepoch_exp = ff.history['dii_per_l1_per_epoch']\n",
    "# # ww0_perl1_perepoch_exp = ff.history['weights_per_l1_per_epoch']\n",
    "# # np.save('./PLOTDATA/num_nonzero2',num_exp)\n",
    "# # np.save('./PLOTDATA/l1o2',lassos_exp)\n",
    "# # np.save('./PLOTDATA/diio2',ii0_exp)\n",
    "# # np.save('./PLOTDATA/weightso2',ww0_exp)\n",
    "# # np.save('./PLOTDATA/dii_per_l1_per_epoch', ii0_perl1_perepoch_exp)\n",
    "# # np.save('./PLOTDATA/weights_per_l1_per_epoch', ww0_perl1_perepoch_exp)\n",
    "\n",
    "# # Cosine decaying learning rate\n",
    "# (\n",
    "#     num_cos,\n",
    "#     lassos_cos,\n",
    "#     ii0_cos,\n",
    "#     ww0_cos,\n",
    "# ) = ff.return_lasso_optimization_dii_search(\n",
    "#     target_data=ff_target,\n",
    "#     l1_penalties=largelassos,\n",
    "#     initial_weights=None,  # (default) set automatically\n",
    "#     n_epochs=n_epochs,\n",
    "#     learning_rate=optilr2, \n",
    "#     decaying_lr=\"exp\",\n",
    "#     refine=False,  # only 10 values of the L1 strength are tested\n",
    "#     plotlasso=True,  # automatically show DII vs number of non-zero features\n",
    "# )\n",
    "# ii0_perl1_perepoch_cos = ff.history['dii_per_l1_per_epoch']\n",
    "# ww0_perl1_perepoch_cos = ff.history['weights_per_l1_per_epoch']\n",
    "# # np.save('./PLOTDATA/COS/num_nonzero2',num_cos)\n",
    "# # np.save('./PLOTDATA/COS/l1o2',lassos_cos)\n",
    "# # np.save('./PLOTDATA/COS/diio2',ii0_cos)\n",
    "# # np.save('./PLOTDATA/COS/weightso2',ww0_cos)\n",
    "# # np.save('./PLOTDATA/COS/dii_per_l1_per_epoch', ii0_perl1_perepoch_cos)\n",
    "# # np.save('./PLOTDATA/COS/weights_per_l1_per_epoch', ww0_perl1_perepoch_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bebf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPONENTIAL decay results\n",
    "ii0_exp = np.load(\"./PLOTDATA/diio2.npy\") ## DIIs for each number of non-zero features\n",
    "ww0_exp = np.load(\"./PLOTDATA/weightso2.npy\") # according weights\n",
    "lassos_exp = np.load(\"./PLOTDATA/l1o2.npy\") # the L1 penalty strength for above results\n",
    "num_exp = np.load(\"./PLOTDATA/num_nonzero2.npy\") # the available numbers of non-zero features\n",
    "ii0_perl1_perepoch_exp = np.load(\"./PLOTDATA/dii_per_l1_per_epoch.npy\") # the optimization for each ii0_exp\n",
    "ww0_perl1_perepoch_exp = np.load(\"./PLOTDATA/weights_per_l1_per_epoch.npy\") # the optimization for each set of ww0_exp\n",
    "\n",
    "#COSINE Annealing results\n",
    "ii0_cos = np.load(\"./PLOTDATA/COS/diio2.npy\")\n",
    "ww0_cos = np.load(\"./PLOTDATA/COS/weightso2.npy\")\n",
    "lassos_cos = np.load(\"./PLOTDATA/COS/l1o2.npy\")\n",
    "num_cos = np.load(\"./PLOTDATA/COS/num_nonzero2.npy\")\n",
    "ii0_perl1_perepoch_cos = np.load(\"./PLOTDATA/COS/dii_per_l1_per_epoch.npy\")\n",
    "ww0_perl1_perepoch_cos = np.load(\"./PLOTDATA/COS/weights_per_l1_per_epoch.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c7e868",
   "metadata": {},
   "source": [
    "## Merge the two sets of results\n",
    "According to lowest DII if they have the same number of non-zero features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120a54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercollate exponential decay resulting DIIs and cosine annealing ones (for Fig.1 AIII)\n",
    "\n",
    "#deep copies of the arrays (by using 1*)\n",
    "ii = 1*ii0_exp\n",
    "ww = 1*ww0_exp\n",
    "lassos = 1*lassos_exp\n",
    "num_nonzero_features = 1*num_exp\n",
    "decaytype = np.full(ii.shape, 0.) #0 is exponential decay\n",
    "decaytype[np.where(np.isnan(ii))] = np.nan\n",
    "\n",
    "\n",
    "for i, number in enumerate(num_exp):\n",
    "    if (num_cos[i] == number and ii0_cos[i] < ii0_exp[i]) or (np.isnan(num_exp[i]) and not np.isnan(num_cos[i])):\n",
    "        decaytype[i] = 1. # 1 means cosine annealing decay\n",
    "        ii[i] = 1*ii0_cos[i]\n",
    "        ww[i] = 1*ww0_cos[i]\n",
    "        lassos[i] = 1*lassos_cos[i]\n",
    "        num_nonzero_features[i] = 1*num_cos[i]\n",
    "        \n",
    "\n",
    "# Extract the dii and weight optimizations for each of the non-zero feature results (for Fig.1 AI)\n",
    "\n",
    "# which L1 strengths were used for the results?\n",
    "lassoindices = np.ones(ii.shape) * np.nan\n",
    "for i, lasso in enumerate(lassos):\n",
    "    j = np.where(np.round(lassos_ori_big,7) == np.round(lasso,7))[0]\n",
    "    if j.size != 0:\n",
    "        lassoindices[i] = j[0]\n",
    "\n",
    "# now extract the correct ones:\n",
    "epochs= ii0_perl1_perepoch_exp.shape[1]\n",
    "features = ww0_perl1_perepoch_exp.shape[2]\n",
    "ii_evolutions2 = np.ones((len(ii), epochs)) * np.nan\n",
    "ww_evolutions2 = np.ones((len(ii), epochs, features)) * np.nan\n",
    "\n",
    "for i, lassind in enumerate(lassoindices):\n",
    "    if decaytype[i] == 0.: #extract from the exponential decay\n",
    "        ii_evolutions2[i] = ii0_perl1_perepoch_exp[int(lassind)]\n",
    "        ww_evolutions2[i] = ww0_perl1_perepoch_exp[int(lassind)]\n",
    "    elif decaytype[i] == 1.: #extract from the cosine decay\n",
    "        ii_evolutions2[i] = ii0_perl1_perepoch_cos[int(lassind)]\n",
    "        ww_evolutions2[i] = ww0_perl1_perepoch_cos[int(lassind)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4880fd",
   "metadata": {},
   "source": [
    "## overlap (cosine similarity) between groundtruth and lasso results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth vector of 285 dimensions, mostly 0's\n",
    "elements = 1*gtlist\n",
    "gtvector = np.zeros(Xori2.shape[1])\n",
    "for i, element in enumerate(elements):\n",
    "    gtvector[element] = weights2[i] \n",
    "\n",
    "# overlap\n",
    "overlap = []\n",
    "for i in range(len(ww)):\n",
    "    overl = np.dot(normalize(ww[i]), normalize(gtvector))\n",
    "    overlap.append(overl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06062a47",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0676aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import NullFormatter\n",
    "fig,ax = plt.subplots(figsize=(4.5, 3.5))\n",
    "features = [0,168,266,274, 277, 279, 281] # selected points to plot\n",
    "\n",
    "# Cosine similarities / overlap\n",
    "nn= np.linalg.norm(ww[features], ord=0, axis=1).astype(int)\n",
    "ov2 = np.array(overlap)[features]\n",
    "ax.plot(nn, ov2, \"o-\",color=\"tab:gray\", linewidth=1)\n",
    "ax.set_xlabel(\"Number of non-zero features\")\n",
    "ax.set_ylabel(\"Overlap of weights\",color=\"tab:gray\")\n",
    "ax.tick_params(axis='y', colors=\"tab:gray\")\n",
    "ax.set_xscale('symlog')\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.xaxis.set_minor_formatter(NullFormatter())\n",
    "ax.set_xticks(num_nonzero_features[features].astype(int), labels=num_nonzero_features[features].astype(int))\n",
    "ax.set_ylim(0.15,1.08)\n",
    "ax.invert_xaxis()\n",
    "ax.set_xlim(330,3)\n",
    "plt.yticks([0.2,0.4,0.6,0.8,1.0])\n",
    "\n",
    "\n",
    "# DIIs\n",
    "endimbalances = ii[features]\n",
    "ax2=ax.twinx()\n",
    "ax2.plot(nn, endimbalances, \"-\",color=\"black\", linewidth=1, zorder=1)\n",
    "colors = ['tab:blue', 'tab:green', 'tab:red', 'tab:cyan','tab:orange', 'tab:brown', 'tab:pink']\n",
    "counter=0\n",
    "for y, c in zip(endimbalances, colors):\n",
    "    plt.scatter(nn[counter], y, color=c)\n",
    "    counter +=1\n",
    "ax2.set_ylabel(\"DII\")\n",
    "ax2.set_ylim(-0.03,0.24)\n",
    "plt.yticks([0.,0.1,0.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeeee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot without L1 regularization and L1=0.0001 results: orange point\n",
    "\n",
    "till=37 # length of plot axis\n",
    "\n",
    "# L1=0.0001\n",
    "num = 277\n",
    "besti0 = 1*ii_evolutions2[num][:till]\n",
    "besti0[till-10:till] = np.nan\n",
    "besti0[till-1] = 1*ii_evolutions2[num][800]\n",
    "# to connect trough missing values:\n",
    "bestimask0 = np.isfinite(besti0)\n",
    "\n",
    "# Unregularized\n",
    "base = 1*ii_evolutions2[0][:till]\n",
    "base[till-10:till] = np.nan\n",
    "base[till-1] = 1*ii_evolutions2[0][800] \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3.5)) #plt.figure(figsize=(5, 4))\n",
    "# Unregularized\n",
    "plt.plot(np.arange(len(base))[bestimask0], base[bestimask0], 'o--', linewidth=1., color=\"tab:blue\")\n",
    "plt.plot(base[:till-9], 'o-',label=lassos[0], linewidth=1.5, color=\"tab:blue\")\n",
    "\n",
    "# L1=0.0001\n",
    "plt.plot(np.arange(len(besti0))[bestimask0], besti0[bestimask0], 'o--', linewidth=1., color=\"tab:orange\")\n",
    "plt.plot(besti0[:till-9], 'o-',label=np.round(lassos[num],4), linewidth=1.5, color=\"tab:orange\")\n",
    "\n",
    "plt.legend(title = \"L$_1$ strength\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"DII\")\n",
    "plt.xlim(-1,till)\n",
    "plt.xticks([0,10,20, 30], labels=[0,10,20,800])\n",
    "plt.yticks([0.,0.05, 0.1,0.15])\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a70859",
   "metadata": {},
   "source": [
    "### the insets of the data point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make pandas for insets with data clouds in Fig.1 BI\n",
    "Xori2pd = pd.DataFrame(Xori2, columns=nameslist)\n",
    "X2pd = pd.DataFrame(X2, columns=nameslist_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c00de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the correct scaling weights for the point clouds\n",
    "num=277\n",
    "gtvector = np.zeros(Xori2.shape[1])\n",
    "for i, element in enumerate(gtlist):\n",
    "    gtvector[element] = weights2[i] \n",
    "# Normalized weights\n",
    "weightslist_gt_0_10_50 = []\n",
    "weightslist_gt_0_10_50.append(normalize(gtvector)) # normalized gt weights\n",
    "#also the weights of the beginning, epoch 3 and epoch 50:\n",
    "optipoints = [0, 9, 800]\n",
    "for i in ww_evolutions2[num][optipoints]:\n",
    "    weightslist_gt_0_10_50.append(normalize(i))   \n",
    "pandaslist = []\n",
    "for i in weightslist_gt_0_10_50:\n",
    "    Xori2_subset = Xori2 * i\n",
    "    Xori2_subsetpd = pd.DataFrame(Xori2_subset[:,[4,100]], columns=['$X_5$', '$X_1X_5X_6$']) #np.array(nameslist)[[20,47]]\n",
    "    pandaslist.append(Xori2_subsetpd)\n",
    "    \n",
    "# Plot   \n",
    "for i in range(1, len(optipoints)+1):\n",
    "    concatenated = pd.concat([pandaslist[0].assign(Weights='Ground truth'), pandaslist[i].assign(Weights='Optimized')])\n",
    "    alphas = np.ones(3000)*1\n",
    "    alphas[100:1500] = 0.8\n",
    "    g = sns.JointGrid(data=concatenated, x=\"$X_5$\", y=\"$X_1X_5X_6$\", height=2, xlim=(-4,4), ylim=(-4,4), hue='Weights', palette=['tab:gray','tab:orange'])\n",
    "    g.plot_joint(sns.scatterplot, s=30, alpha=alphas, linewidth=0)\n",
    "    g.plot_marginals(sns.kdeplot, fill=True, alpha=0.2, linewidth=2)\n",
    "    g.ax_joint.tick_params(left=False, bottom=False, labelbottom=False, labelleft=False)\n",
    "    g.ax_joint.legend_.remove()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf5e8b4",
   "metadata": {},
   "source": [
    "### For the table in Fig.1 B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844aadf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtlist_sorted = [4, 100, 2, 20, 5, 9, 11, 280, 7, 47] #ground truth indices sorted descending by weight\n",
    "\n",
    "otherelements = list(set(np.arange(285)) - set(gtlist))\n",
    "optiweights = (ww.T/np.max(ww, axis=1)).T * 10\n",
    "features = [0,168,266,274, 277, 279, 281, 284]\n",
    "for i, feat in enumerate (features):\n",
    "    print(\"l1: \",np.round(lassos[feat],7),\" norm: \", num_nonzero_features[feat].astype(int), \" imb: \", np.round(ii[feat],3),\" sum rest: \", np.round(np.nansum(optiweights[feat,otherelements]),1), \" weights: \", np.round(optiweights[feat, gtlist_sorted],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35793b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
