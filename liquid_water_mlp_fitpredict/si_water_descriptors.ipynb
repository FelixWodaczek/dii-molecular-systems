{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Robustness of Feature Weighting on Water Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import re\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from ase.io import read as ase_read\n",
    "\n",
    "from dadapy.feature_weighting import FeatureWeighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "Large SOAP datasets and especially ACSFs are somewhat slow to calculate, so I mostly just run this on our local cluster and then load from numpy files.\n",
    "To recalculate the descriptors, use `make_descriptors.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(\"../data\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ase.Atoms objects for each liquid configuration\n",
    "liquid_frames = ase_read(data_dir.joinpath(\"ice_in_water_data/dataset_1000_eVAng.xyz\"), index=':')\n",
    "n_atoms = np.sum(np.asarray([len(frame) for frame in liquid_frames], dtype=np.int16))\n",
    "atom_types = np.zeros((n_atoms), dtype=np.int8)\n",
    "\n",
    "# Collect some metadata, like how many atoms/config, atoms in total and which atom is even an oxygen.\n",
    "counter = 0\n",
    "for frame in liquid_frames:\n",
    "    atom_types[counter:counter+len(frame)] = frame.get_atomic_numbers()\n",
    "    counter+=len(frame)\n",
    "is_o = atom_types==8\n",
    "is_h = np.logical_not(is_o)\n",
    "\n",
    "print(f\"Found {np.count_nonzero(is_o)} Oxygen atoms and {np.count_nonzero(is_h)} Hydrogen atoms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new descriptors from file or laod ase.Atoms objects and recalculate\n",
    "# Recalculation here works best when just getting SOAPs, ACSF takes a little long\n",
    "average_soap = np.load(data_dir.joinpath(\"ice_in_water_data/average_soap_rcut6_nmax6_lmax6_sigma03.npy\"))\n",
    "atomic_soap = np.load(data_dir.joinpath(\"ice_in_water_data/singleatom_soap_rcut6_nmax6_lmax6_sigma03.npy\"))\n",
    "print(\"Fetched computed atomic SOAP descriptors for %u configurations and with %u features each.\"%atomic_soap.shape)\n",
    "print(\"Fetched computed global SOAP descriptors for %u configurations and with %u features each.\"%average_soap.shape)\n",
    "\n",
    "# The file format of the input file the descriptors are calculated from is 54 solid, 1000 liquid\n",
    "# So we can just get the liquid configurations by getting the number of atoms n_atoms in the liquid configurations\n",
    "# From the end of the decriptor matrix\n",
    "liquid_atomic_soap = atomic_soap[-n_atoms:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_acsf = np.asarray(np.load(data_dir.joinpath(\"ice_in_water_data/average_acsf_rcut6_gridsearch_bohr_lambda.npy\")), dtype=np.float32)\n",
    "atomic_acsf = np.asarray(np.load(data_dir.joinpath(\"ice_in_water_data/singleatom_acsf_rcut6_gridsearch_bohr_lambda.npy\")), dtype=np.float32)\n",
    "liquid_atomic_acsf = atomic_acsf[-n_atoms:, :].copy()\n",
    "print(\"Fetched computed atomic SOAP descriptors for %u configurations and with %u features each.\"%atomic_acsf.shape)\n",
    "print(\"Fetched computed liquid atomic SOAP descriptors for %u configurations and with %u features each.\"%liquid_atomic_acsf.shape)\n",
    "print(\"Fetched computed global SOAP descriptors for %u configurations and with %u features each.\"%average_acsf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Processing of Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = [average_soap, atomic_soap, liquid_atomic_soap, average_acsf, atomic_acsf, liquid_atomic_acsf]\n",
    "for desc in descriptors:\n",
    "    desc /= np.linalg.norm(desc, axis=-1)[:, np.newaxis]\n",
    "average_soap, atomic_soap, liquid_atomic_soap, average_acsf, atomic_acsf, liquid_atomic_acsf = descriptors\n",
    "\n",
    "# apparently atomic acsf sometimes become nan, set to 0\n",
    "nan_frames = np.argwhere(np.isnan(atomic_acsf))[:, 0]\n",
    "print(\"Removing %u nan frames in atomic acsf\"%(len(np.unique(nan_frames))))\n",
    "atomic_acsf[nan_frames, :] = 0.\n",
    "\n",
    "nan_frames = np.argwhere(np.isnan(liquid_atomic_acsf))[:, 0]\n",
    "print(\"Removing %u nan frames in liquid atomic acsf\"%(len(np.unique(nan_frames))))\n",
    "liquid_atomic_acsf[nan_frames, :] = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Input and Target Data\n",
    "\n",
    "`target_data` is the space which delivers the target ranks, `input_space` is the space for which the gammas should be optimised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "# random_selection = rng.choice(liquid_atomic_soap.shape[0], 300)\n",
    "target_data = liquid_atomic_soap[::500]\n",
    "input_space = liquid_atomic_acsf[::500]\n",
    "\n",
    "fw_input = FeatureWeighting(coordinates=input_space, maxk=input_space.shape[0]-1, verbose=True)\n",
    "fw_target = FeatureWeighting(coordinates=target_data, maxk=target_data.shape[0]-1, verbose=True)\n",
    "\n",
    "if True:\n",
    "    stds = np.std(input_space, axis=0)\n",
    "    stds[stds==0.] = 1.\n",
    "    standardised_input = input_space/stds[np.newaxis, :]\n",
    "    # dist_matrix = kimb.compute_dist_matrix(data=target_data.astype(np.double), period=np.zeros((target_data.shape[-1])))\n",
    "    # truth_ranks = stats.rankdata(dist_matrix, method='average', axis=1).astype(int, copy=False)\n",
    "\n",
    "print(f\"Working on ground truth shaped {target_data.shape} and optimising space of shape {input_space.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Optimal Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_gammas = np.ones((input_space.shape[-1],), dtype=np.double)\n",
    "lr_list = np.logspace(-2, 2, 10)\n",
    "\n",
    "opt_l_rate = fw_input.return_optimal_learning_rate(\n",
    "    target_data=fw_target, initial_weights=initial_gammas, lambd=None, \n",
    "    n_epochs=50, decaying_lr=True, \n",
    "    n_samples=154, trial_learning_rates=lr_list,\n",
    ")\n",
    "\n",
    "lr_kernel_imbs = fw_input.history['dii_per_epoch_per_lr'][np.argwhere(opt_l_rate==lr_list)[0, 0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "ax.plot(lr_kernel_imbs, label=\"kernel imbalances\")\n",
    "\n",
    "ax.set_title(\"Loss for optimal learning rate %f\"%opt_l_rate)\n",
    "# ax.set_yscale('log')\n",
    "ax.set_ylabel('Kernel Imbalance')\n",
    "ax.set_xlabel(\"epoch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise using Lasso Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    num_nonzero_features,\n",
    "    l1_penalties_opt_per_nfeatures,\n",
    "    dii_opt_per_nfeatures, \n",
    "    weights_opt_per_nfeatures\n",
    ") = fw_input.return_lasso_optimization_dii_search(\n",
    "    target_data=fw_target, initial_weights=initial_gammas, lambd=None, \n",
    "    learning_rate=opt_l_rate, constrain=True, decaying_lr=True,\n",
    "    n_epochs=100, refine=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert order so it has least features first\n",
    "kernel_imbs = dii_opt_per_nfeatures[::-1]\n",
    "lasso_gammas = weights_opt_per_nfeatures[::-1]\n",
    "num_nonzero_features = num_nonzero_features[::-1]\n",
    "l1_penalties_opt_per_nfeatures = l1_penalties_opt_per_nfeatures[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O for Precomputed Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'load_old'\n",
    "\n",
    "if mode == 'load_old':\n",
    "    num_nonzero_features = np.loadtxt(data_dir.joinpath('water_phase_store/robustness_num_nonzero_features.txt'))\n",
    "    l1_penalties_opt_per_nfeatures = np.loadtxt(data_dir.joinpath('water_phase_store/robustness_l1_penalties_opt_per_nfeatures.txt'))\n",
    "    kernel_imbs = np.loadtxt(data_dir.joinpath('water_phase_store/robustness_kernel_imbs.txt'))\n",
    "    lasso_gammas = np.loadtxt(data_dir.joinpath('water_phase_store/robustness_lasso_gammas.txt'))\n",
    "if mode == 'save_new':\n",
    "    np.savetxt(data_dir.joinpath('water_phase_store/robustness_num_nonzero_features.txt'), num_nonzero_features)\n",
    "    np.savetxt(data_dir.joinpath('water_phase_store/robustness_l1_penalties_opt_per_nfeatures.txt'), l1_penalties_opt_per_nfeatures)\n",
    "    np.savetxt(data_dir.joinpath('water_phase_store/robustness_kernel_imbs.txt'), kernel_imbs)\n",
    "    np.savetxt(data_dir.joinpath('water_phase_store/robustness_lasso_gammas.txt'), weights_opt_per_nfeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness\n",
    "\n",
    "### Matching weights\n",
    "Take $L_1$ penalty from weighting with certain number of features and test if same features (or maybe only with a similar value of DII) are selected by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_true_feat = 10\n",
    "true_weights = lasso_gammas[n_true_feat-1]\n",
    "true_l1 = l1_penalties_opt_per_nfeatures[n_true_feat-1]\n",
    "\n",
    "assert np.count_nonzero(true_weights) == n_true_feat, \"Wrong number of features selected.\"\n",
    "\n",
    "n_compare = 30\n",
    "compare_weights = lasso_gammas[n_compare-1]\n",
    "compare_l1 = l1_penalties_opt_per_nfeatures[n_compare-1]\n",
    "\n",
    "assert np.count_nonzero(compare_weights) == n_compare, \"Wrong number of features selected.\"\n",
    "\n",
    "print(kernel_imbs.shape)\n",
    "plt.plot(kernel_imbs)\n",
    "plt.xlim(0, 25)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(l1_penalties_opt_per_nfeatures)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datas = [50, 70, 100, 150, 200, 250, 300, 350]\n",
    "\n",
    "weights_per_ndata = []\n",
    "dii_compare = []\n",
    "\n",
    "for n_data in n_datas:\n",
    "    test_indices = rng.choice(liquid_atomic_soap.shape[0], n_data)\n",
    "\n",
    "    fw_input = FeatureWeighting(coordinates=liquid_atomic_acsf[test_indices], maxk=n_data-1, verbose=True)\n",
    "    fw_target = FeatureWeighting(coordinates=liquid_atomic_soap[test_indices], maxk=n_data-1, verbose=True)\n",
    "\n",
    "    initial_gammas = np.ones((input_space.shape[-1],), dtype=np.double)\n",
    "    weights = fw_input.return_weights_optimize_dii(\n",
    "        target_data=fw_target, initial_weights=initial_gammas, lambd=None,\n",
    "        learning_rate=None, l1_penalty=compare_l1,\n",
    "        constrain=True, decaying_lr=True,\n",
    "        n_epochs=100\n",
    "    )\n",
    "    print(f\"Optimised weights for {n_data} data points.\")\n",
    "    print(f\"DII: {fw_input.history['dii_per_epoch'][-1]}\")\n",
    "    dii_compare.append(fw_input.history['dii_per_epoch'][-1])\n",
    "    \n",
    "    weights_per_ndata.append(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "ax.plot([np.count_nonzero(weights) for weights in weights_per_ndata], label=\"kernel imbalances\")\n",
    "plt.show()\n",
    "\n",
    "where_true = np.argwhere(true_weights)\n",
    "for weights in weights_per_ndata:\n",
    "    where_weights = np.argwhere(weights)\n",
    "    \n",
    "    print(f\"Found {np.count_nonzero(weights)} features.\")\n",
    "    print(f\"{len(np.intersect1d(where_true, where_weights))} are in top 10.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training multiple Networks\n",
    "\n",
    "As it turns out, collecting weights for a different number of features only partially reproduces the weights gathered by the \"full\" dataset of 380 features.\n",
    "Therefore, run the DII optimisation on multiple different number of features and reproduce predictions there to see differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'save_new'\n",
    "\n",
    "n_datas = np.logspace(6.5, 8, 4, base=2).astype(np.int16)\n",
    "print(f'Running for {n_datas} number of points')\n",
    "\n",
    "dii_per_nfeatures_ndata = []\n",
    "weights_per_nfeatures_ndata = []\n",
    "for n_data in n_datas:\n",
    "    if mode == 'save_new':\n",
    "        test_indices = rng.choice(liquid_atomic_soap.shape[0], n_data)\n",
    "\n",
    "        fw_input = FeatureWeighting(coordinates=liquid_atomic_acsf[test_indices], maxk=n_data-1, verbose=True)\n",
    "        fw_target = FeatureWeighting(coordinates=liquid_atomic_soap[test_indices], maxk=n_data-1, verbose=True)\n",
    "\n",
    "        (\n",
    "            num_nonzero_features,\n",
    "            l1_penalties_opt_per_nfeatures,\n",
    "            dii_opt_per_nfeatures, \n",
    "            weights_opt_per_nfeatures\n",
    "        ) = fw_input.return_lasso_optimization_dii_search(\n",
    "            target_data=fw_target, initial_weights=initial_gammas, lambd=None, \n",
    "            learning_rate=None, constrain=True, decaying_lr=True,\n",
    "            n_epochs=100, refine=True\n",
    "        )\n",
    "        dii_per_nfeatures = dii_opt_per_nfeatures[::-1]\n",
    "        weights_per_nfeatures = weights_opt_per_nfeatures[::-1]\n",
    "        \n",
    "        np.savetxt(data_dir.joinpath(f'water_phase_store/dii_per_nfeatures_ndata_{n_data}.txt'), dii_per_nfeatures)\n",
    "        np.savetxt(data_dir.joinpath(f'water_phase_store/weights_per_nfeatures_ndata_{n_data}.txt'), weights_per_nfeatures)\n",
    "    elif mode == 'load_old':\n",
    "        dii_per_nfeatures = np.loadtxt(data_dir.joinpath(f'water_phase_store/dii_per_nfeatures_ndata_{n_data}.txt'))\n",
    "        weights_per_nfeatures = np.loadtxt(data_dir.joinpath(f'water_phase_store/weights_per_nfeatures_ndata_{n_data}.txt'))\n",
    "    dii_per_nfeatures_ndata.append(dii_per_nfeatures)\n",
    "    weights_per_nfeatures_ndata.append(weights_per_nfeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "\n",
    "target_dir = data_dir.joinpath(\"n2p2_fitting/240205_training_out/\")\n",
    "\n",
    "results_dict = {}\n",
    "for logfile in target_dir.glob('*.log'):\n",
    "    with open(logfile, 'r') as f:\n",
    "        content = f.read()\n",
    "        runtype = re.search(\"231213_pot_acsf(.*)_hartbohr_scaleunits_bcdata_lambda\", content).group(1)\n",
    "        memory_kb = re.search(\"\\s([0-9]*)maxresident\", content).group(1)\n",
    "        runtime_s = re.search(\"([0-9]*)user\", content).group(1)\n",
    "        \n",
    "        pot_dir = target_dir.joinpath(\"231213_pot_acsf\"+runtype+\"_hartbohr_scaleunits_bcdata_lambda/\")\n",
    "        all_errors = np.loadtxt(pot_dir.joinpath('learning-curve.out'))\n",
    "        all_errors[:, 1:9] *= 27.211386245988\n",
    "        all_errors[:, 9:13] *= 51.421 # Hartree/Bohr to eV/Angstrom conversion\n",
    "        rmse_ftest_evA = all_errors[-1, 12]\n",
    "        \n",
    "        results_dict[runtype] = {\"memory\": int(memory_kb), \"runtime\": float(runtime_s), \"rmse\": rmse_ftest_evA}\n",
    "print(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "target_dir = data_dir.joinpath(\"n2p2_fitting/240724_pot_si_ndatas_out/\")\n",
    "\n",
    "results_dict_si = {}\n",
    "for potdir in target_dir.glob('ndata*'):\n",
    "    ndata = re.match(r'ndata\\_(\\d+)\\_nfeat\\_10', potdir.name).group(1)\n",
    "    all_errors = np.loadtxt(potdir.joinpath('learning-curve.out'))\n",
    "    all_errors[:, 1:9] *= 27.211386245988\n",
    "    all_errors[:, 9:13] *= 51.421 # Hartree/Bohr to eV/Angstrom conversion\n",
    "    rmse_ftest_evA = all_errors[-1, 12]\n",
    "    \n",
    "    results_dict_si[ndata] = {\"rmse\": rmse_ftest_evA}\n",
    "print(results_dict_si)\n",
    "\n",
    "rmses_res = []\n",
    "ndatas_res = []\n",
    "for ndata, res in results_dict_si.items():\n",
    "    ndatas_res.append(int(ndata))\n",
    "    rmses_res.append(res['rmse'])\n",
    "\n",
    "sorter_ = np.argsort(ndatas_res)\n",
    "ndatas_res = np.array(ndatas_res)[sorter_]\n",
    "rmses_res = np.array(rmses_res)[sorter_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "fontsize = 12\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n",
    "\n",
    "ax.plot(ndatas_res, rmses_res, label='SI')\n",
    "ax.plot(384, results_dict['10']['rmse'], 'ro', label=r'$DII$')\n",
    "ax.plot(384, results_dict['10rand']['rmse'], 'rs', label=r'random')\n",
    "ax.set_xscale('log')\n",
    "\n",
    "ax.xaxis.set_major_formatter(ticker.NullFormatter())\n",
    "\n",
    "ax.tick_params(axis='x', which='both', bottom=False, labelbottom=False)\n",
    "ax.set_xticks(ndatas_res, labels=[6.5, 7, 7.5, 8], minor=False, fontsize=fontsize)\n",
    "ax.tick_params(axis='x', which='major', bottom=True, labelbottom=True)\n",
    "\n",
    "ax.set_ylabel(\"Test Force RMSE [eV/A]\", fontsize=fontsize)\n",
    "ax.set_xlabel(r\"Number of Data Points in $DII$ selection\" , fontsize=fontsize)\n",
    "ax.legend(fontsize=fontsize)\n",
    "\n",
    "fig.savefig('rmse_vs_ndata_dii.png', dpi=300, format='png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dadapy_usage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
